# ğŸ¾ Tennis Match Prediction - Machine Learning Project

[![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/downloads/)

> **PredicciÃ³n de Ganadores en Partidos de Tenis ATP usando Machine Learning Avanzado**

Proyecto de Aprendizaje AutomÃ¡tico (APA) - Q1 2025-26  
**Autores:** Oriol FarrÃ©s & Marc Gil

---

## ğŸ“– DescripciÃ³n del Proyecto

Este proyecto desarrolla un **sistema completo de predicciÃ³n de partidos de tenis** del circuito ATP masculino utilizando tÃ©cnicas avanzadas de Machine Learning. A partir de datos histÃ³ricos de 2011-2024 (39,542 partidos), construimos modelos predictivos capaces de superar a sistemas profesionales como **IBM Watson** en torneos especÃ­ficos.

### ğŸ¯ Objetivos Principales

1. **PredicciÃ³n binaria**: Â¿QuiÃ©n ganarÃ¡ el partido? (Player 1 vs Player 2)
2. **Feature Engineering riguroso**: CreaciÃ³n de 80+ variables predictivas
3. **ValidaciÃ³n temporal**: TimeSeriesSplit para evitar data leakage
4. **Benchmarking profesional**: ComparaciÃ³n con IBM Watson en Roland Garros 2024

---

## ğŸ† Resultados Destacados

| MÃ©trica | Valor | Contexto |
|---------|-------|----------|
| **Best CV Score** | **70.65%** | XGBoost con Optuna (2011-2020) |
| **Test Score** | **69.8%** | GeneralizaciÃ³n en datos 2021-2024 |
| **Roland Garros 2024** | **80%** | Walk-forward validation ronda a ronda |
| **IBM Watson Benchmark** | ~70% | Nuestro modelo supera en Grand Slams |

### ğŸ“Š Modelos Implementados

- âœ… **Lineales**: Logistic Regression, Linear SVM, K-Nearest Neighbors
- âœ… **No Lineales**: Random Forest, XGBoost, Neural Networks (MLP)
- âœ… **Ensemble**: Voting Classifier, Stacking Classifier
- âœ… **OptimizaciÃ³n**: Optuna para hyperparameter tuning (250+ trials por modelo)

---

## ğŸš€ CÃ³mo Ejecutar el Proyecto

### ğŸ“‹ Requisitos del Sistema

- **OS**: Windows/Linux/MacOS
- **RAM**: MÃ­nimo 16GB (Recomendado: 32GB + 20GB swap)
- **GPU**: Opcional (XGBoost puede usar CUDA)
- **Tiempo estimado**: ~3 horas en HP Omen 16 (i7, 32GB RAM, RTX 3060)

### 1ï¸âƒ£ ConfiguraciÃ³n Inicial

Sigue la [GuÃ­a de ConfiguraciÃ³n](SETUP.md) para instalar dependencias:

```bash
# Clonar repositorio
git clone git@github.com:Sterrysx/Tennis-ML-APA.git
cd Tennis-ML-APA

# Crear entorno virtual
conda create -n tennis-ml python=3.11 -y
conda activate tennis-ml

# Instalar dependencias
pip install -r requirements.txt

# TambiÃ©n se puede crear el entorno virtual siguiendo las instrucciones de SETUP.md

# Configurar kernel de Jupyter
python -m ipykernel install --user --name=tennis-ml --display-name="Python (tennis-ml)"
```

### 2ï¸âƒ£ EjecuciÃ³n Secuencial de Notebooks

**Orden obligatorio:**

```bash
# OPCIONAL: Descargar datos desde cero (ya incluidos en el repo)
./pull_data.sh

# 1. Preprocesado (30-60 min)
jupyter notebook notebooks/01_preprocesado.ipynb

# 2. Modelado y EvaluaciÃ³n (2-4 horas)
jupyter notebook notebooks/02_ajuste_y_conclusiones.ipynb

# 3. Benchmarking Roland Garros (15-30 min)
jupyter notebook notebooks/03_extra_IBM.ipynb
```

### âš™ï¸ ConfiguraciÃ³n de Recursos

Si tienes **memoria limitada**, ajusta estos parÃ¡metros en los notebooks:

```python
# En 02_ajuste_y_conclusiones.ipynb

# Reducir trials de Optuna
n_iter=50  # En lugar de 250-500

# Reducir cores usados
n_jobs=4   # En lugar de -1 (todos los cores)
```

---

## ğŸ“‚ Estructura del Proyecto

```
Tennis-ML-APA/
â”‚
â”œâ”€â”€ notebooks/                      # ğŸ““ Jupyter Notebooks principales
â”‚   â”œâ”€â”€ 00_preamble.ipynb          # IntroducciÃ³n y contexto
â”‚   â”œâ”€â”€ 01_preprocesado.ipynb      # Limpieza y feature engineering
â”‚   â”œâ”€â”€ 02_ajuste_y_conclusiones.ipynb  # Modelos ML y evaluaciÃ³n
â”‚   â”œâ”€â”€ 03_extra_IBM.ipynb         # Benchmarking Roland Garros
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ raw/                    # Datos originales
â”‚       â”‚   â””â”€â”€ raw_atp_matches.csv
â”‚       â”œâ”€â”€ clean/                  # Datos procesados
â”‚       â”‚   â”œâ”€â”€ atp_matches_train.parquet
â”‚       â”‚   â”œâ”€â”€ atp_matches_test.parquet
â”‚       â”‚   â””â”€â”€ ibm.parquet
â”‚       â””â”€â”€ ibm/                    # Roland Garros 2024 splits
â”‚           â”œâ”€â”€ train_step_1_R128.parquet
â”‚           â”œâ”€â”€ val_step_1_R128.parquet
â”‚           â””â”€â”€ ...
â”‚
â”œâ”€â”€ pull-data/                      # ğŸ“¥ Scripts de descarga de datos
â”‚   â”œâ”€â”€ atp_matches/               # CSVs originales por aÃ±o
â”‚   â”‚   â”œâ”€â”€ atp_matches_2011.csv
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â””â”€â”€ atp_matches_2024.csv
â”‚   â”œâ”€â”€ parsed_data/               # Datos intermedios
â”‚   â”‚   â””â”€â”€ atp_matches_2011_2024.csv
â”‚   â””â”€â”€ src/                       # Scripts Python
â”‚       â”œâ”€â”€ 01_download_data.py
â”‚       â”œâ”€â”€ 02_merge.py
â”‚
â”œâ”€â”€ requirements.txt                # ğŸ“¦ Dependencias Python
â”œâ”€â”€ pull_data.sh                   # ğŸ”§ Script de descarga automÃ¡tica
â”œâ”€â”€ SETUP.md                       # ğŸ“– GuÃ­a de instalaciÃ³n
â””â”€â”€ README.md                      # ğŸ“˜ Este archivo
```

---

## ğŸ§ª MetodologÃ­a TÃ©cnica

### ğŸ”¬ Feature Engineering (80+ Variables)

#### **CaracterÃ­sticas Temporales**
- **ELO Rating**: Sistema estÃ¡ndar de ajedrez adaptado a tenis
  - ELO Global (`elo`)
  - ELO por Superficie (`elo_surface`: Clay/Grass/Hard)
  - ELO Blended: `(elo + elo_surface) / 2`
  - **Margin of Victory (MoV)**: Multiplicador basado en diferencia de juegos
    - FÃ³rmula: `K * log(game_diff + 1) * 0.6`
    - Ejemplo: Victoria 6-0, 6-0 â†’ Multiplicador ~2.5x

#### **EstadÃ­sticas Bayesianas (Suavizado C=10)**
Para cada ventana temporal (last_1, last_5, last_10, lifetime):
- `win_rate`: Ratio de victorias
- `ace_pct`: Porcentaje de aces
- `df_pct`: Porcentaje de dobles faltas
- `1st_won_pct`: Efectividad primer servicio
- `bp_save_pct`: Salvamento de break points
- `tb_rate`: Frecuencia de tiebreaks
- `tb_won_pct`: Victorias en tiebreaks

**LÃ³gica de Rookies vs Veteranos:**
- Rookies (ranking > 200 en primera apariciÃ³n): ImputaciÃ³n conservadora
- Veteranos (ranking < 200): Suavizado Bayesiano hasta 10 partidos

#### **Variables Contextuales**
- `h2h`: Head-to-Head histÃ³rico (victorias previas)
- `is_seeded`: Â¿Es cabeza de serie?
- `is_first_match`: Â¿Es su debut profesional? (con filtro de veteranos)
- `days_since`: DÃ­as desde Ãºltimo partido (detecta lesiones si > 90 dÃ­as)
- `diff_*`: Diferencias entre jugadores (rank, elo, h2h, stats...)

### ğŸ¯ ValidaciÃ³n sin Data Leakage

#### **Temporal Split Estricto**
```python
# Train: 2011-2020 (70% - 25,125 partidos)
# Test:  2021-2024 (30% - 10,921 partidos)
```

#### **TimeSeriesSplit para CV**
```python
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)
# Cada fold respeta el orden cronolÃ³gico
# Fold 1: Train [2011-2014] â†’ Val [2015]
# Fold 2: Train [2011-2016] â†’ Val [2017]
# ...
```

#### **GarantÃ­a de No-Leakage**
- Todas las features histÃ³ricas usan `.shift(1)` en pandas
- H2H se calcula ANTES de actualizar el registro
- ELO pre-partido se guarda ANTES de aplicar el delta
- Variables in-match eliminadas: `ace`, `df`, `svpt`, `minutes`, `score`

### ğŸ¤– OptimizaciÃ³n con Optuna

**Hyperparameter Tuning Bayesiano:**
- **Tree-Based Optimization**: Optuna usa TPE (Tree-structured Parzen Estimator)
- **Early Stopping**: Pruning automÃ¡tico de trials no prometedores
- **Distribuciones Continuas**: FloatDistribution con escala logarÃ­tmica

**Ejemplo XGBoost:**
```python
param_dist_xgb = {
    'n_estimators': IntDistribution(100, 600),
    'learning_rate': FloatDistribution(0.005, 0.3, log=True),
    'max_depth': IntDistribution(3, 10),
    'subsample': FloatDistribution(0.5, 1.0),
    'gamma': FloatDistribution(0, 5.0),
    'reg_alpha': FloatDistribution(1e-8, 10.0, log=True),
}
```

---

## ğŸ“Š AnÃ¡lisis de Resultados

### ğŸ… Ranking de Modelos (por CV Score)

| PosiciÃ³n | Modelo | CV Score | Test Score | Diferencia | AUC |
|----------|--------|----------|------------|------------|-----|
| ğŸ¥‡ 1Âº | **XGBoost** | **70.65%** | 69.8% | +0.85% | 0.780 |
| ğŸ¥ˆ 2Âº | Stacking Ensemble | 70.50% | 69.5% | +1.00% | 0.775 |
| ğŸ¥‰ 3Âº | Voting Ensemble | 70.33% | 69.3% | +1.03% | 0.772 |
| 4Âº | MLP Neural Network | 69.85% | 68.9% | +0.95% | 0.765 |
| 5Âº | SVM con Kernel | 69.94% | 69.1% | +0.84% | 0.768 |
| 6Âº | Random Forest | 69.60% | 68.7% | +0.90% | 0.760 |
| 7Âº | Linear SVM | 68.02% | 67.5% | +0.52% | 0.745 |
| 8Âº | Logistic Regression | 68.19% | 67.8% | +0.39% | 0.748 |
| 9Âº | K-Nearest Neighbors | 67.73% | 67.2% | +0.53% | 0.742 |

### ğŸ’¡ Insights Clave

1. **XGBoost domina** gracias a su capacidad de capturar interacciones no lineales
2. **Ensemble Methods** mejoran marginalmente pero aÃ±aden complejidad computacional
3. **Baja diferencia CV-Test** (~0.5-1%) indica **NO overfitting**
4. **Modelos lineales** alcanzan ~68% (muy respetable para un problema tan complejo)

### ğŸ¾ Variables MÃ¡s Importantes (XGBoost)

Top 10 Features por Gain:
1. `diff_elo_blend` - Diferencia de ELO combinado (global + superficie)
2. `diff_rank` - Diferencia de ranking ATP
3. `diff_win_rate_last_10` - Forma reciente (Ãºltimos 10 partidos)
4. `diff_elo_surface` - Diferencia de ELO especÃ­fico de superficie
5. `diff_h2h` - Head-to-Head histÃ³rico
6. `diff_1st_won_pct_lifetime` - Efectividad primer servicio (carrera)
7. `diff_bp_save_pct_last_5` - Salvamento de BPs (Ãºltimos 5 partidos)
8. `diff_days_since` - Diferencia de inactividad (frescura)
9. `diff_ace_pct_last_10` - Potencia al servicio reciente
10. `diff_tb_won_pct_lifetime` - Clutch mental en tiebreaks

---

## ğŸ”¬ Experimento Roland Garros 2024

### ğŸ¯ Objetivo
Validar el modelo en condiciones **real-world** contra el predictor oficial de IBM Watson.

### ğŸ“ˆ Resultados por Ronda

| Ronda | Partidos | Accuracy | Confianza Media | Upsets Detectados |
|-------|----------|----------|-----------------|-------------------|
| R128 | 64 | 75.0% | 0.68 | 16 |
| R64 | 32 | 78.1% | 0.71 | 7 |
| R32 | 16 | 81.3% | 0.74 | 3 |
| R16 | 8 | 75.0% | 0.69 | 2 |
| QF | 4 | 100% | 0.81 | 0 |
| SF | 2 | 100% | 0.85 | 0 |
| F | 1 | 100% | 0.58 | 0 (Alcaraz vs Zverev) |
| **TOTAL** | **127** | **~80%** | **0.72** | **28** |

### ğŸ“Š AnÃ¡lisis

**Â¿Por quÃ© 80% en RG vs 70% CV general?**

1. **Grand Slams son mÃ¡s predecibles**
   - Top 32 seeded (protecciÃ³n de favoritos)
   - Best-of-5 sets (reduce varianza)
   - MotivaciÃ³n mÃ¡xima (reduce upsets)

2. **Clay-specific features brillan**
   - `elo_surface` captura especializaciÃ³n en tierra
   - Roland Garros = Torneo mÃ¡s predecible en clay

3. **Walk-forward learning**
   - Modelo se adapta partido a partido
   - Incorpora forma del torneo en tiempo real

4. **Varianza estadÃ­stica**
   - 127 partidos vs 25,000 de CV (muestra pequeÃ±a)
   - Intervalo de confianza: [75%-85%] es razonable

**ComparaciÃ³n con IBM Watson:**
- IBM Watson: ~70% reported accuracy
- Nuestro modelo: ~80% en Roland Garros
- **Ventaja**: +10% en Grand Slams especÃ­ficos

---

## ğŸ“ˆ MÃ©tricas de Rendimiento

### â±ï¸ Tiempos de EjecuciÃ³n (HP Omen 16)

**Especificaciones de prueba:**
- CPU: Intel i7-11800H (8 cores)
- RAM: 32GB DDR4 + 20GB swap
- GPU: NVIDIA RTX 3060 (6GB VRAM - no usado por XGBoost CPU)
- SSD: NVMe Gen4

| Notebook | Tiempo | CPU Uso | RAM Pico | Nota |
|----------|--------|---------|----------|------|
| 00_preamble | ~1 min | 20% | 2GB | Carga inicial |
| 01_preprocesado | 30-60 min | 90% | 8GB | Feature engineering intensivo |
| 02_ajuste_y_conclusiones | 2-4 horas | 95% | 12GB | Optuna + mÃºltiples modelos |
| 03_extra_IBM | 15-30 min | 80% | 6GB | Walk-forward RG 2024 |
| **TOTAL** | **~3-5 horas** | - | - | EjecuciÃ³n completa |

### ğŸ’¾ Espacio en Disco

- **Datos raw**: ~11 MB (`raw_atp_matches.csv`)
- **Datos procesados**: ~5 MB (parquet files)
- **Modelos guardados**: ~200 MB (pipelines con preprocesadores)
- **Notebooks ejecutados**: ~50 MB (con outputs)
- **Total proyecto**: ~300 MB


---

## ğŸ“š Referencias y Fuentes de Datos

### ğŸ“Š Datos

- **Jeff Sackmann** - tennis_atp repository  
  GitHub: [JeffSackmann/tennis_atp](https://github.com/JeffSackmann/tennis_atp)  
  License: CC BY-NC-SA 4.0

### ğŸ† Benchmarks

- **IBM Watson Tennis**: Predictor oficial de Grand Slams
  - [IBM Sports & Entertainment](https://www.ibm.com/sports)
  - Accuracy reportado: ~70% en Grand Slams

---

**Ãšltima actualizaciÃ³n:** Enero 2026  
